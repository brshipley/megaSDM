% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nullAUC.R
\name{nullAUC}
\alias{nullAUC}
\title{Generate null distribution models for effective AUC comparison}
\usage{
nullAUC(envdata, replicates = 50, bufflist = NA, modelpar)
}
\arguments{
\item{envdata}{a RasterStack or list of raster files corresponding to the
area the model will be trained on.}

\item{replicates}{how many times should the null model be run to get a distribution
of AUC values? Default is 50 replicates.}

\item{bufflist}{(optional) if background points were spatially-constrained, provide
the paths to the buffer files (.shp) used.}

\item{modelpar}{a list of the arguments passed from the \code{MaxEntModel} function.
These arguments should be exactly the same as the actual model generation for effective
comparison. For the method described by Bohl et al. (2019), a list of test-samples must
be given (evaluate models on the real data). Otherwise, the null distribution will be
evaluated on a subset of the random samples (see Raes & ter Steege (2007))}
}
\value{
A .csv file (NullModel_AUC.csv) with the Test AUC values for each replicate
of the null model.
}
\description{
AUC values rely on both omission rate (false negatives) and commision rate (false positives);
however, MaxEnt is a presence-only method, making raw AUC values uninformative for
comparing across models and species (Jimenez-Valverde 2012). One way to use AUC values to
examine presence-only model predictions is to generate model replicates using randomly-generated
occurrence data, evaluating their performance using a subset of the real occurrence data.
This function generates null models and calculates the Test AUC values when applied to
the subset of real occurrence data, for comparison with the model training on the actual
data. This method was developed by Bohl et al. 2019.
}
